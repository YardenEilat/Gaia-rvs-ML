{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import system\n",
    "import umap\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "from os import system\n",
    "import os.path\n",
    "import platform\n",
    "import requests\n",
    "import random\n",
    "from astropy.io import fits\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy\n",
    "import scipy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD RVS FLUXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system('wget -r --no-parent -nH http://cdn.gea.esac.esa.int/Gaia/gdr3/Spectroscopy/rvs_mean_spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system('gunzip -d -r Gaia/gdr3/Spectroscopy/rvs_mean_spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def read_rvs_csv(file_path):\n",
    "    df = pd.read_csv(file_path, skiprows=83)\n",
    "    return df\n",
    "\n",
    "def save_ids_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, columns=['source_id'], index=False)\n",
    "    \n",
    "spectra_folder_path = 'Gaia/gdr3/Spectroscopy/rvs_mean_spectrum'\n",
    "full_df = pd.DataFrame()\n",
    "for file in os.listdir(spectra_folder_path):\n",
    "    if file.endswith('csv'):\n",
    "        print(f'{file}')\n",
    "        df_temp = read_rvs_csv(os.path.join(spectra_folder_path, file))[['source_id', 'flux']]\n",
    "        full_df = full_df.append(df_temp, ignore_index=True)\n",
    "        \n",
    "        \n",
    "full_df.to_pickle('rvs_flux_dataframe.pkl')\n",
    "save_ids_to_csv(full_df, 'rvs_source_ids.csv')\n",
    "\n",
    "print('DONE')\n",
    "\n",
    "# for i in range(len(indexes)):\n",
    "#     with fits.open('/storage/home/yardenei/gaia/fits/' + str(data.apogee_id[indexes[i]])+'.fits') as temp_fits:\n",
    "#         wl_size=8575\n",
    "#         vwave = numpy.zeros(wl_size) # vacuum wl\n",
    "#         awave = numpy.zeros(wl_size) # air wl\n",
    "#         wl_delta = float(temp_fits[1].header['CDELT1'])\n",
    "#         wl_logstart = float(temp_fits[1].header['CRVAL1'])\n",
    "#         start_pix = int(temp_fits[1].header['CRPIX1'])\n",
    "#         #cur_wl_size = numpy.min([wl_size,int(hdulist[0].header['NWAVE'])])\n",
    "#         vwave[0] = 10**wl_logstart\n",
    "#         for j in range (start_pix, wl_size):\n",
    "#             vwave[j] = vwave[j-1]*(10**wl_delta)\n",
    "#             awave = vwave / (1.0 +  5.792105E-2/(238.0185E0 - (1.E4/vwave)**2) + 1.67917E-3/( 57.362E0 - (1.E4/vwave)**2))\n",
    "#         flux1 = temp_fits[1].data.copy()\n",
    "#         flux2 = temp_fits[2].data.copy()\n",
    "#         flux3 = temp_fits[3].data.copy()\n",
    "#     if i==0:\n",
    "#         df_flux_arr = pd.DataFrame({'pcn':[flux1],'fit':[flux3]})\n",
    "#     else:\n",
    "#         df_flux_arr=df_flux_arr.append(pd.DataFrame({'pcn':[flux1],'fit':[flux3]}),ignore_index=True)\n",
    "#     print(str(i/len(indexes)*100) + '% done')\n",
    "#     gc.collect()\n",
    "# fluxes = empty((len(indexes), len(concatenate((arange(246,3274),arange(3585,6080),arange(6344,8335) )))))\n",
    "# fit = empty((len(indexes), len(concatenate((arange(246,3274),arange(3585,6080),arange(6344,8335) )))))\n",
    "# #wavelengths = empty((len(indexes), len(concatenate((arange(246,3274),arange(3585,6080),arange(6344,8335) )))))\n",
    "# for i in arange(len(indexes)):\n",
    "#     fluxes[i] = concatenate((df_flux_arr.pcn[i][246:3274],df_flux_arr.pcn[i][3585:6080],df_flux_arr.pcn[i][6344:8335]), axis=None)\n",
    "#     fit[i] = concatenate((df_flux_arr.fit[i][246:3274],df_flux_arr.fit[i][3585:6080],df_flux_arr.fit[i][6344:8335]), axis=None)\n",
    "#     #wavelengths[i] = concatenate((df_flux_arr.awave[i][246:3274],df_flux_arr.awave[i][3585:6080],df_flux_arr.awave[i][6344:8335]), axis=None)\n",
    "# ##Replace None values by the mean\n",
    "# #imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "# #fluxes = imp.fit_transform(fluxes)\n",
    "# #%%\n",
    "\n",
    "# savez('fluxes.npz', fluxes = fluxes, fit = fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD ASTROPHYSICAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system('wget -r --no-parent -nH http://cdn.gea.esac.esa.int/Gaia/gdr3/Astrophysical_parameters/astrophysical_parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system('gunzip -d -r Gaia/gdr3/Astrophysical_parameters/astrophysical_parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def read_rvs_csv(file_path):\n",
    "    df = pd.read_csv(file_path, skiprows=1541)\n",
    "    return df\n",
    "\n",
    "def save_ids_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, columns=['source_id'], index=False)\n",
    "    \n",
    "spectra_folder_path = 'Gaia/gdr3/Astrophysical_parameters/astrophysical_parameters'\n",
    "full_df = pd.DataFrame()\n",
    "for file in os.listdir(spectra_folder_path):\n",
    "    if file.endswith('csv'):\n",
    "        print(f'{file}')\n",
    "        df_temp = read_rvs_csv(os.path.join(spectra_folder_path, file))[['source_id', 'flux']]\n",
    "        full_df = full_df.append(df_temp, ignore_index=True)\n",
    "        \n",
    "        \n",
    "full_df.to_pickle('astrophysical_parameters_dataframe.pkl')\n",
    "# save_ids_to_csv(full_df, 'rvs_source_ids.csv')\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter the spectra to the range of 150:2250 and remove objects with nan values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_pickle('rvs_flux_dataframe.pkl')\n",
    "df_filtered = df.copy()\n",
    "df_filtered.set_index(\"source_id\", inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "import numpy as np\n",
    "params = pd.read_csv('gaia_params_lite.csv')\n",
    "rv = params.loc[:,'radial_velocity']\n",
    "rv_source_id = params.loc[:,'source_id']\n",
    "min_lambda = 846\n",
    "max_lambda = 870\n",
    "wavelength = np.linspace(min_lambda, max_lambda, len(eval(df['flux'][0].replace('NaN', 'np.nan'))))\n",
    "c = 299792\n",
    "from scipy.interpolate import interp1d\n",
    "ids_list = list()\n",
    "for i in range(len(df)):\n",
    "    if np.mod(i,1000)==0:\n",
    "        print(f'{i/len(df)*100}%')\n",
    "    try:\n",
    "        flux = eval(df['flux'][i].values[0].replace('NaN', 'np.nan'))\n",
    "    except:\n",
    "        flux = eval(df['flux'][i].replace('NaN', 'np.nan'))\n",
    "    if np.isnan(flux[150:2250]).any():\n",
    "        ids_list.append(df['source_id'][i])\n",
    "df_filtered = df_filtered.drop(labels=ids_list)\n",
    "\n",
    "\n",
    "print(f'{len(df_filtered)/len(df)*100}% objects left after deletion')\n",
    "source_ids = np.empty(len(df_filtered))\n",
    "fluxes = np.empty((len(df_filtered), len(range(150, 2250))))\n",
    "snr = np.empty(source_ids.shape)\n",
    "for i in range(len(df_filtered)):\n",
    "    df_temp = df_filtered.iloc[i]\n",
    "    source_ids[i] = df_filtered.index[i]\n",
    "    flux = np.array(eval(df_temp.loc['flux'].replace('NaN', 'np.nan')))\n",
    "    fluxes[i] = flux[150:2250]\n",
    "    if np.mod(i, 1000) == 0:\n",
    "        print(f'creating numpy arrays {i/len(df)*100}%')\n",
    "np.savez('filtered_nans_fluxes_and_ids.npz', fluxes=fluxes, source_ids=source_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rvs_flux_dataframe.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4952/4050290133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rvs_flux_dataframe.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# df.set_index(range(len(df)), inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msource_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     ) as handles:\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rvs_flux_dataframe.pkl'"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('rvs_flux_dataframe.pkl')\n",
    "source_ids = np.empty(len(df))\n",
    "fluxes = np.empty((len(df), len(range(15, 2250))))\n",
    "flux_errors = np.empty(fluxes.shape)\n",
    "snr = np.empty(source_ids.shape)\n",
    "for i in range(len(df)):\n",
    "    source_ids[i] = df.index[i]\n",
    "    fluxes[i] = np.array(eval(df['flux'][source_ids[i]].replace('NaN', 'np.nan')))[15:2250]\n",
    "    flux_errors[i] = np.array(eval(df['flux_error'][source_ids[i]].replace('NaN', 'np.nan')))[15:2250]\n",
    "    snr[i] = np.median(fluxes[i]/flux_errors[i])\n",
    "    if np.mod(i, 1000) == 0:\n",
    "        print(f'{i/len(df)*100}%')\n",
    "np.savez('filtered_fluxes_and_ids.npz', fluxes=fluxes, flux_errors=flux_errors, snr=snr, source_ids=source_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pylab import *\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#load fluxes\n",
    "with np.load('filtered_nans_fluxes_and_ids.npz',allow_pickle=True) as f:\n",
    "    source_ids = f['source_ids']\n",
    "    random_indexes = random.sample(range(len(source_ids)), int(len(source_ids)*0.2))\n",
    "    fluxes = f['fluxes'][random_indexes]\n",
    "print(f'number of objects in dataset = {len(source_ids)}')\n",
    "# data = pd.read_csv('JOIN_APOGEE_GAIA.csv')\n",
    "# indexes = arange(len(data))\n",
    "\n",
    "#create sample of fakes with same distribution (sampled from actual spectrums)\n",
    "marginal_dist_values = []\n",
    "marginal_dist_weights = []\n",
    "for w in range(len(fluxes[0])):\n",
    "    h = np.histogram(fluxes[:,w],100)\n",
    "    values = np.empty(len(h[0]))\n",
    "    for i in range(len(h[0])):\n",
    "        values[i] = np.mean((h[1][i], h[1][i+1]))\n",
    "    values = np.delete(values, np.argwhere(h[0]==0))\n",
    "    weights = np.delete(h[0],np.argwhere(h[0]==0))\n",
    "    marginal_dist_values.append(values)\n",
    "    marginal_dist_weights.append(weights)\n",
    "\n",
    "num_fakes = int(len(random_indexes)*0.5)\n",
    "fake_fluxes = np.empty((num_fakes, len(fluxes[0])))\n",
    "\n",
    "for i in range(num_fakes):\n",
    "    for w in range(len(fluxes[0])):\n",
    "        fake_fluxes[i][w] = np.double(random.choices(marginal_dist_values[w], marginal_dist_weights[w]))\n",
    "    if np.mod(i, 1000) == 0:\n",
    "        print('creating fake fluxes.. ' + str(i/num_fakes*100) + '% done.')\n",
    "total_fluxes = np.empty((len(fluxes)+len(fake_fluxes),len(fluxes[0,:])))\n",
    "total_fluxes[0:len(fluxes)] = fluxes\n",
    "total_fluxes[len(fluxes):len(total_fluxes)] = fake_fluxes\n",
    "is_true_flux = np.zeros(len(total_fluxes))\n",
    "is_true_flux[0:len(fluxes[:,0])] = 1\n",
    "is_true_flux=is_true_flux.astype(int)\n",
    "\n",
    "np.savez('rvs_fluxes_with_fakes_for_training.npz', total_fluxes = total_fluxes, is_true_flux = is_true_flux, source_ids=source_ids[random_indexes])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m96"
  },
  "kernelspec": {
   "display_name": "gaia-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "54ed4bf4e0f79fa21de1a4b699b29bbadaea68304c4d671acd9679a9eab88722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
